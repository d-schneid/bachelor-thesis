\subsection{Reconstruction Error}
Since the discretized time series resulted from the time series discretization algorithms are represented by alphabet symbols, they need to be transformed into a numerical representation in order to be comparable with the corresponding original standardized time series. This transformation is called numerical reconstruction \cite{APCA}. After performing the numerical reconstruction, the reconstruction error measures the goodness of approximation of the reconstructed numerical representation of the discretized time series with respect to the corresponding original standardized time series \cite{APCA}. Since the time series discretization algorithms presented in this thesis do not inherently define a numerical reconstruction, the first task is to define such. Subsequently, the second task is to define the measurement of the reconstruction error (i.e. the goodness of approximation).
\subsection*{General Numerical Reconstruction}
Let $X = x_1, ..., x_N$ be a standardized time series of length $N \geq 1$. Assume that $\hat{X} = \hat{x}_1, ..., \hat{x}_n$ is the corresponding discretized time series resulted from one of the time series discretization algorithms applied on $X$ with a length of $1 \leq n \leq N$. Suppose that $A := \{\alpha_j \mid j \in \{1, ..., a \} \}$ is the corresponding alphabet with $a \geq 2$ alphabet symbols such that $\hat{x}_i \in A \ (1 \leq i \leq n)$. Then, the first step of reconstructing $X$ from $\hat{X}$ is to determine a function $f: A \rightarrow \mathbb{R}$ that maps each alphabet symbol $\alpha_j \in A$ to a numerical value \cite{Comparison_SAX}. Applying $f$ on each $\hat{x}_i$ results in an intermediate time series $\tilde{X}^{'} = \tilde{x}_1, ..., \tilde{x}_n$. However, this is not the final result of the numerical reconstruction if $n < N$, because the reconstructed numerical time series needs to have a length of $N$ to be comparable with $X$. Therefore, each $\tilde{x}_i \ (1 \leq i \leq n)$ is concatenated $w$ times with itself to obtain $\tilde{x}_{i}^{1}, ..., \tilde{x}_{i}^{w}$ as a reconstructed numerical subsequence for the $i$th subsequence that was extracted by the \ac{PAA} while discretizing $X$ into $\hat{X}$ based on a window length of $w \geq 1$ \cite{Comparison_SAX}. Thus, the final reconstructed numerical time series is $\tilde{X} = \tilde{x}_{1}^{1}, ..., \tilde{x}_{1}^{w}, ..., \tilde{x}_{n}^{1}, ..., \tilde{x}_{n}^{w}$ with a length of $w \cdot n = N$ (see Figure \ref{fig:num_recon}).
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{evaluation/reconstruction_error/numerical_recon.pdf}
\caption[Reconstruction Error - Numerical Reconstruction]{Based on the corresponding discretized time series resulted from a time series discretization algorithm, the original standardized time series is reconstructed. Since this numerical reconstruction is only an approximation of the original standardized time series, the reconstruction error measures the goodness of this approximation \cite{APCA}.}
\label{fig:num_recon}
\end{figure}
\subsection*{Strategies for Determining $f: A \rightarrow \mathbb{R}$} \label{function_strategies}
Let $B := (\beta_0,\beta_1,...,\beta_{a-1},\beta_a)$ be the ascending sorted list of breakpoints computed by a time series discretization algorithm with $\beta_0 = -\infty$ and $\beta_a = +\infty$. Then define $d_j := [\beta_{j-1},\beta_j) \ (1 \leq j \leq a)$ as the corresponding discretization intervals. \newline
The first strategy for determining the function $f$ is based on the points of the original standardized time series $X$. Based on Equation \ref{eq:SAX_Discretization}, each point $x_i \ (1 \leq i \leq n)$ is assigned to the discretization interval $d_j$ it is located in. But, instead of discretizing these points, the mean $\overline{d}_j$ of the points in each $d_j$ is computed. Then, $f$ is determined by mapping each alphabet symbol $\alpha_j$ to the mean of its corresponding discretization interval: $f(\alpha_j) = \overline{d}_j \ (1 \leq j \leq a)$ \cite{Comparison_SAX}. \newline
The second strategy for determining the function $f$ is a variant of the first strategy (see Subfigure \ref{fig:strategy_mean}). Instead of computing the mean of the points in each $d_j$, the median is computed. Thus, these medians $f(\alpha_j)$ are more robust to outliers within the corresponding discretization intervals $d_j$ compared to the means from the first strategy. \newline
Instead of considering the points of the original standardized time series, the third strategy considers the discretization intervals itself. For this strategy, the function $f$ is determined by mapping each alphabet symbol $\alpha_j$ to the midpoint of its corresponding discretization interval such that $f(\alpha_j) = (\beta_{j-1} + \beta_j) / 2$ \cite{Survey_Temporal_Discretization}. However, there is one pitfall for the computation of the midpoints $f(\alpha_1)$ and $f(\alpha_a)$. Since $\beta_0 = -\infty$ and $\beta_a = +\infty$, these midpoints are not defined. One possibility to cope with this pitfall is to set the values of the minimum and maximum point of the original standardized time series $X$ as artificial breakpoints $\beta_0$ and $\beta_a$, respectively. This way, the midpoints $f(\alpha_1)$ and $f(\alpha_a)$ are defined and can be computed like the other midpoints (see Subfigure \ref{fig:strategy_midpoint}).
\begin{figure}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{evaluation/reconstruction_error/strategy_mean.pdf}
\caption{For the first and second strategy, the points of the original standardized time series (green) are assigned to the respective discretization interval (a, b, c) they are located in. Within each discretization interval, the mean/median of the points is computed. This value represents the numerical value (f(a), f(b), f(c)) for the alphabet symbol that is assigned to the corresponding discretization interval \cite{Comparison_SAX}.}
\label{fig:strategy_mean}
\end{subfigure}
\hfill%
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{evaluation/reconstruction_error/strategy_midpoint.pdf}
\caption{For the third strategy, the midpoint of each discretization interval (a, b, c) is computed. This value represents the numerical value (f(a), f(b), f(c)) for the alphabet symbol that is assigned to the corresponding discretization interval \cite{Survey_Temporal_Discretization}. Since the first and last discretization interval are not lower respectively upper bounded, the values of the minimum and maximum point of the original standardized time series are set as artificial breakpoints for computing the respective midpoint.}
\label{fig:strategy_midpoint}
\end{subfigure}
\caption[Reconstruction Error - Numerical Values for Alphabet Symbols]{Strategies for determining numerical values for alphabet symbols to perform the numerical reconstruction.}
\label{fig:function_strategies}
\end{figure}
\subsection*{Pecularities for Multiple Alphabet Symbols per Subsequence}
While the explained numerical reconstruction along with the determination of the function $f$ can be applied without modifications for the time series discretization algorithms that use one alphabet symbol for representing an extracted subsequence by the \ac{PAA}, there are pecularities for the \ac{eSAX} and \ac{1d-SAX}. \newline
Let $\hat{X} = sort(\{\hat{min}_1, \hat{x}_1, \hat{max}_1\}), ..., sort(\{\hat{min}_n, \hat{x}_n, \hat{max}_n\})$ be the \ac{eSAX} representation of $X$. Recall that $\hat{X}$ contains three alphabet symbols per extracted subsequence. For the numerical reconstruction of $X$ from $\hat{X}$, the determination and application of the function $f$ does not need to be modified. Thus, the intermediate time series $\tilde{X}^{'} = sort(\{\tilde{min}_1, \tilde{x}_1, \tilde{max}_1\}), ..., sort(\{\tilde{min}_n, \tilde{x}_n, \tilde{max}_n\})$ is obtained by applying $f$ on each alphabet symbol of $\hat{X}$. However, to obtain the final reconstructed numerical time series $\tilde{X}$ (see Subfigure \ref{fig:e_sax_reconstructed}), each $\tilde{x}_i \ (1 \leq i \leq n)$ is first concatenated $w$ times with itself to obtain $\tilde{x}_{i}^{1}, ..., \tilde{x}_{i}^{w}$, as explained previously. But in addition, those positions in time that correspond to the positions in time of the minimum and maximum point in the $i$th extracted subsequence by the \ac{PAA}, are overwritten with $\tilde{min}_i$ and $\tilde{max}_i$, respectively. As a an example, consider the concatenation $\tilde{x}_{i}^1, \tilde{x}_{i}^2, \tilde{x}_{i}^3, \tilde{x}_{i}^4, \tilde{x}_{i}^5$ of $\tilde{x}_i$ based on a window length of $w = 5$. For obtaining $\tilde{X}$, this concatenation would further be transformed to $\tilde{x}_{i}^1, \tilde{max}_{i}, \tilde{x}_{i}^3, \tilde{x}_{i}^4, \tilde{min}_{i}$, assuming the maximum and minimum point in the $i$th extracted subsequence are at positions 2 and 5, respectively. Note that for $w = 2$, the numerical reconstruction of an extracted subsequence is only based on the minimum and maximum point of that subsequence while ignoring its mean. Further, for $w = 1$, the mean, minimum and maximum point of a subsequence are equal. \newline
For the \ac{1d-SAX}, the determination and application of the function $f$ needs to be modified. Recall that the \ac{1d-SAX} representation $\hat{X} = (\hat{x}_1,\hat{s}_1), ..., (\hat{x}_n,\hat{s}_n)$ of $X$ contains two alphabet symbols per extracted subsequence by the \ac{PAA}. While for the other time series discretization algorithms one function $f$ is sufficient, for the \ac{1d-SAX} two different functions of this kind are needed. The reason is that the values of the means and slopes of the points of the extracted subsequences by the \ac{PAA} are separateley discretized based on different discretization intervals \cite{1d-SAX}. Therefore, the strategies for determining $f$ need to be adapted to these different discretization intervals. Let $f^m: A^m \rightarrow \mathbb{R}$ and $f^s: A^s \rightarrow \mathbb{R}$ be the two different functions to be determined based on the alphabets $A^m$ and $A^s$ that are used for discretizing the means and slopes, respectively. While $f^m$ can be determined based on any of the three strategies described above, the first and second strategy need to be modified for determining $f^s$. For these two strategies, instead of using the points of the original standardized time series, the computed slope values of the extracted subsequences by the \ac{PAA} that shall be discretized need to be used. Further, note that for the third strategy, the respective discretization intervals for the means or the slopes need to be used depending if $f^m$ or $f^s$ shall be determined. After determining $f^m$ and $f^s$, the intermediate time series $\tilde{X}^{'} = (\tilde{x}_1,\tilde{s}_1), ..., (\tilde{x}_n,\tilde{s}_n)$ is then obtained by separately applying $f^m$ and $f^s$ on each $\hat{x}_i$ and $\hat{s}_i \ (1 \leq i \leq n)$, respectively \cite{1d-SAX}. \newline
Finally, let $t_{i}^{1}, ..., t_{i}^{w}$ be the positions in time of the $i$th extracted subsequence by the \ac{PAA} based on a window length of $w$. Then, the numerical reconstruction for this subsequence is obtained based on the formula \cite{1d-SAX}:
\begin{equation*}
\tilde{x}_{i}^{j} = \tilde{x}_i + \tilde{s}_i \cdot (t_{i}^{j} - t_{i}^{mid}) \qquad (1 \leq j \leq w),
\end{equation*}
where $t_{i}^{mid} = (t_{i}^{1} + t_{i}^{w}) / 2$. Applying this formula for each extracted subsequence by the \ac{PAA}, results in the final reconstructed numerical time series $\tilde{X} = \tilde{x}_{1}^{1}, ...,\tilde{x}_{1}^{w}, ...,\tilde{x}_{n}^{1}, ...,\tilde{x}_{n}^{w}$ of length $w \cdot n = N$ (see Subfigure \ref{fig:one_d_sax_reconstructed}) \cite{1d-SAX}.
\begin{figure}
\centering
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{evaluation/reconstruction_error/e_sax_reconstructed.pdf}
\caption{The reconstructed numerical time series $\tilde{X}$ based on the discretization of the original standardized time series $X$ with the \ac{eSAX}.}
\label{fig:e_sax_reconstructed}
\end{subfigure}
\par \bigskip
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{evaluation/reconstruction_error/one_d_sax_reconstructed.pdf}
\caption{The reconstructed numerical time series $\tilde{X}$ based on the discretization of the original standardized time series $X$ with the \ac{1d-SAX}.}
\label{fig:one_d_sax_reconstructed}
\end{subfigure}
\caption[Reconstruction Error - Numerical Reconstruction based on the eSAX and 1d-SAX]{The reconstructed numerical time series $\tilde{X}$ are plotted against the corresponding original standardized time series $X$. Performing the numerical reconstruction on the discretized time series $\hat{X}$ results in the reconstructed numerical time series $\tilde{X}$ \cite{APCA}.}
\label{fig:reconstructed_e_sax_one_d_sax}
\end{figure}
\subsection*{Measuring the Reconstruction Error}
The reconstruction error of the reconstructed numerical time series $\tilde{X} = \tilde{x}_1, ..., \tilde{x}_N$ with respect to the original standardized time series $X = x_1, ..., x_N$ is measured based on the deviation of the pairwise points of $X$ and $\tilde{X}$ at the same position in time (see Figure \ref{fig:measuring_recon_error}). Thus, for each point of $\tilde{X}$, the goodness of its reconstruction with respect to the corresponding point of $X$ is measured. \newline
One possible measure that measures the average goodness of these point deviations is the \ac{MAE} \cite{Comparison_SAX}:
\begin{equation*}
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |x_i - \tilde{x}_i|.
\label{eq:MAE}
\end{equation*}
Another possible measure for measuring the average goodness of these point deviations is the \ac{MSE} \cite{T_SAX}:
\begin{equation*}
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \tilde{x}_i)^2
\label{eq:MSE}
\end{equation*}
While the point deviations are equally weighted in the \ac{MAE}, the \ac{MSE} gives relatively larger weights to relatively greater point deviations due to the squaring operation. But, both measures should be minimized for maximizing the goodness of approximation of the reconstructed numerical time series $\tilde{X}$ with respect to the original standardized time series $X$.
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{evaluation/reconstruction_error/measuring_recon_error.pdf}
\caption[Reconstruction Error - Measuring the Reconstruction Error]{The measurement of the reconstruction error is indicated by connecting the pairwise points of the original standardized time series $X$ and the reconstructed numerical time series $\tilde{X}$ at the same position in time. The deviation of these pairwise points is measured for measuring the reconstruction error \cite{Comparison_SAX,T_SAX}.}
\label{fig:measuring_recon_error}
\end{figure}
\subsection*{Datasets Used for Experimental Evaluation}
The dataset labeled \texttt{Gaussian} contains 100 time series with 1,000 points each. These points were drawn from the standard normal distribution $\mathcal{N}(0,1)$. \newline
The dataset labeled \texttt{Non-Gaussian} contains 500 time series with 1,000 points each. For each time series, five different values $\mu_i \in \{-4, -3, ..., 3, 4\} \ (1 \leq i \leq 5)$ and five different values $\sigma_i \in \{0.1, 0.2, ..., 0.8, 0.9\} \ (1 \leq i \leq 5)$ were randomly drawn. Then, a random number of points, but at least 20, were successively drawn from each Gaussian distribution $\mathcal{N}(\mu_i,\sigma_i) \ (1 \leq i \leq 5)$, such that the resulting concatenated time series consists of 250 points in total. This time series was concatenated four times with itself to obtain the final time series with 1,000 points (see Figure \ref{fig:synth_repeated}). \newline
For the dataset labeled \texttt{UCR}, 100 time series were randomly drawn from each of the training datasets named FordB, FordA, FaceAll, FacesUCR, Plane, SwedishLeaf, and OSULeaf (see Table \ref{tab:shapiro_UCR}) from the UCR Time Series Classification Archive \cite{UCR_Archive}. Thus, this dataset contains 700 time series in total. \newline
The dataset labeled \texttt{Motif} contains 500 synthetic time series with 5,000 points each. These time series contain motifs of fixed length that are randomly placed between random walks. In total, the dataset is composed of 100 time series per motif length of 50, 100, 150, 200, and 250 points. \newline
All time series were standardized before performing the experimental evaluation.
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{evaluation/reconstruction_error/synth_repeated.pdf}
\caption[Reconstruction Error - Synthetic Repeating Time Series]{Two exemplary time series from the dataset labeled \texttt{Non-Gaussian}. Every 250 points, the time series are repeated based on the respective pattern that is composed of the points that were successively drawn from five different Gaussian distributions.}
\label{fig:synth_repeated}
\end{figure}
\subsection*{Configurations Used for Experimental Evaluation}
The time series discretization algorithms were experimentally evaluated based on their respective main procedure presented in Chapter \ref{chap:ts_discretization}. However, for the \ac{aSAX} and Persist presented modifications were applied. The \ac{aSAX} was evaluated based on the modification of computing individual adapted breakpoints for each time series instead of discretizing all time series of a dataset based on the same adapted breakpoints (see Subsection \ref{a_sax_modfications}). This modification was applied to obtain a more accurate measurement of the reconstruction error for each time series. The Persist was evaluated based on the modification of discretizing the points of the \ac{PAA} representation (i.e. the extracted means) instead of the points of the corresponding original standardized time series (see Subsection \ref{persist_modifications}). This modification was applied to increase the comparability of the Persist with respect to the other time series discretization algorithms. \newline
Furthermore, for an uniform numerical reconstruction, the function $f: A \rightarrow \mathbb{R}$ was determined based on the first strategy presented (see Subsection \ref{function_strategies}). The results of the experimental evaluation are qualitatively consistent for the other two strategies.   
\subsection*{Experimental Results for Different Window Lengths} \label{findings_different_window_length}
The time series discretization algorithms presented in Chapter \ref{chap:ts_discretization} were experimentally evaluated with respect to the reconstruction error based on different window lengths used for the \ac{PAA} (see Table \ref{tab:recon_error_window_length}). For a concise presentation of the corresponding experimental results, the reconstruction error was measured based on the \ac{MAE}, since the results are qualitatively consistent when measuring the reconstruction error based on the \ac{MSE}. The mean of the MAE across all time series of the respective dataset is presented as experimental result. Moreover, for discretizing the features of subsequences extracted by the \ac{PAA}, an alphabet size of $a = 5$ was used for computing the breakpoints except for the extracted slopes in the \ac{1d-SAX}. For these, an alphabet size of $a = 3$ was used, since in the literature it is recommended to use a smaller alphabet size for the extracted slopes compared to that for the extracted means \cite{1d-SAX}. An experimental evaluation for other alphabet sizes was conducted as well (see Table \ref{tab:recon_error_alphabet_size}). \newline
For all evaluated time series discretization algorithms, the overall experimental results based on different window lengths used for the \ac{PAA} show that the reconstruction error increases with an increasing window length. This finding is in line with the considerations in Subsection \ref{parameter_window_length} about the goodness of approximation of a time series by its \ac{PAA} representation. \newline
For the \texttt{Gaussian} dataset, the \ac{eSAX} results in the lowest reconstruction error followed by the \ac{1d-SAX} for all evaluated window lengths. While the other evaluated time series discretization algorithms only use the means of the subsequences extracted by the \ac{PAA}, the \ac{eSAX} uses the minimum and maximum points and the \ac{1d-SAX} uses the slopes as additional information for discretization. This indicates why these two result in the two lowest reconstruction errors for all evaluated window lengths. Moreover, for all evaluated window lengths, the \ac{SAX} and \ac{aSAX} result in similar reconstruction errors and the Persist results in the largest reconstruction error. Another finding is that the respective reconstruction error converges for all time series discretization algorithms as the window length increases. Based on the considerations in Subsection \ref{effect_paa}, the reason for this convergence is the distributions of the means and slopes of the subsequences extracted by the \ac{PAA}. Since the time series of the \texttt{Gaussian} dataset follow the standard normal distribution $\mathcal{N}(0,1)$, the distributions of the extracted means and slopes have a mean of zero and decreasing standard deviations with an increasing window length. Therefore, as the window length increases, the extracted means and slopes start to concentrate in the respective discretization intervals near the value of zero. Hence, the numerical reconstruction, and therefore the reconstruction error, becomes stable with an increasing window length. The convergence of the reconstruction error also applies for the \ac{eSAX}. However, it is slower due to the arbitrarily located minimum and maximum points across discretization intervals, that slow down the described concentration effect towards the discretization intervals near the value of zero. But, it still converges, because the number of subsequences extracted by the \ac{PAA} decreases with an increasing window length, and therefore, the number of extracted minimum and maximum points decreases as well. \newline
The first finding for the \texttt{Non-Gaussian} dataset is that the \ac{aSAX} and Persist are competitive to the \ac{eSAX} and \ac{1d-SAX} up to the evaluated window length of $w = 20$. However, this is also in line with the considerations in Chapter \ref{chap:ts_discretization}, as the \ac{aSAX} and Persist are classified as adaptive-breakpoints discretization algorithms. In contrast to the other evaluated time series discretization algorithms, these two adapt their breakpoints to the distributions of the time series of the \texttt{Non-Gaussian} dataset. Another finding is that the \ac{eSAX} results in the lowest reconstruction error up to the evaluated window length of $w = 20$, while the \ac{1d-SAX} results in the lowest reconstruction error for $w = 40$. On the one hand, the minimum and maximum point of a subsequence extracted by the \ac{PAA} become less significant for the numerical reconstruction of the subsequence as the window length (i.e. the number of points of the subsequence) increases. Therefore, with respect to the reconstruction error, the benefit of additionally using the minimum and maximum point for discretization, as for the \ac{eSAX}, decreases with an increasing window length. On the other hand, with the slope of a subsequence, the \ac{1d-SAX} uses a global property of the subsequence that is computed based on all points of the subsequence in contrast to considering only two extreme points as for the \ac{eSAX}. Therefore, with respect to the reconstruction error based on a window length of $w = 40$, it seems that additionally using the slope as a global property of a subsequence is advantageous compared to additionally using only two extreme points of a subsequence. \newline
For the \texttt{UCR} dataset, this change in leadership already occurs at the evaluated window length of $w = 10$. While the \ac{eSAX} results in the lowest reconstruction error up to the evaluated window length of $w = 5$, the \ac{1d-SAX} results in the lowest reconstruction error from the evaluated window length of $w = 10$ up to the largest evaluated window length of $w = 40$. This finding can be explained with the same argumentation as for the \texttt{Non-Gaussian} dataset. Another finding is that the \ac{aSAX} results in the largest reconstruction error for a window length of $w = 40$, while for the smaller evaluated window lengths it is similar to the \ac{SAX} and lower than the Persist. The reason for this finding is that 400 out of 700 time series in the \texttt{UCR} dataset contain about 130 to 140 points. Applying a window length of $w = 40$ then implies the clustering of $130 / 40 \approx 3$ points (i.e. means) of the \ac{PAA} representation for computing the individual adapted breakpoints for the corresponding time series based on the \ac{aSAX}. Therefore, the quality of these computed breakpoints is degraded for discretization. Thus, the \ac{aSAX} should only be applied for \ac{PAA} representations that consist of an appropriate number of points (i.e. means). Based on the experimental results, the \ac{PAA} representations resulted from a window length of $w = 20$ seem to have an appropriate number of points. Another solution is to apply the described modification of clustering the points of the corresponding original standardized time series instead of the points of the \ac{PAA} representation (see Subsection \ref{a_sax_modfications}). \newline
As discussed before, the experimental results for the \texttt{Motif} dataset also show that the \ac{1d-SAX} results in a lower reconstruction error for larger evaluated window lengths starting from $w = 20$ compared to the \ac{eSAX}. \newline
\begin{table}[htb]
\centering
\begin{tabular}{cccccc} 
\toprule
& \multicolumn{5}{c}{\ac{MAE}} \\
\cmidrule(lr){2-6}
$a = 5$ & $w = 2$ & $w = 5$ & $w = 10$ & $w = 20$ & $w = 40$ \\
\midrule
\textbf{\texttt{Gaussian}} &  &  &  &  & \\
\ac{SAX} & 0.59 & 0.73 & 0.77 & 0.79 & 0.80 \\
\ac{eSAX} & 0.23 & 0.39 & 0.54 & 0.65 & 0.73 \\
\ac{1d-SAX} & 0.37 & 0.66 & 0.74 & 0.77 & 0.79 \\
\ac{aSAX} & 0.59 & 0.73 & 0.78 & 0.80 & 0.82 \\
Persist & 0.65 & 0.78 & 0.82 & 0.84 & 0.85 \\
\midrule
\textbf{\texttt{Non-Gaussian}} &  &  &  &  & \\
\ac{SAX} & 0.21 & 0.25 & 0.27 & 0.35 & 0.47 \\
\ac{eSAX} & 0.17 & 0.19 & 0.23 & 0.31 & 0.43 \\
\ac{1d-SAX} & 0.18 & 0.23 & 0.26 & 0.32 & 0.40 \\
\ac{aSAX} & 0.19 & 0.22 & 0.25 & 0.34 & 0.47 \\
Persist & 0.19 & 0.22 & 0.25 & 0.34 & 0.47 \\
\midrule
\textbf{\texttt{UCR}} &  &  &  &  & \\
\ac{SAX} & 0.26 & 0.36 & 0.50 & 0.66 & 0.73 \\
\ac{eSAX} & 0.22 & 0.26 & 0.39 & 0.57 & 0.68 \\
\ac{1d-SAX} & 0.23 & 0.29 & 0.38 & 0.52 & 0.64 \\
\ac{aSAX} & 0.24 & 0.35 & 0.50 & 0.67 & 0.79 \\
Persist & 0.29 & 0.39 & 0.53 & 0.68 & 0.75 \\
\midrule
\textbf{\texttt{Motif}} &  &  &  &  & \\
\ac{SAX} & 0.21 & 0.24 & 0.26 & 0.31 & 0.39 \\
\ac{eSAX} & 0.20 & 0.21 & 0.23 & 0.28 & 0.36 \\
\ac{1d-SAX} & 0.20 & 0.22 & 0.23 & 0.26 & 0.32 \\
\ac{aSAX} & 0.20 & 0.22 & 0.25 & 0.30 & 0.38 \\
Persist & 0.23 & 0.25 & 0.28 & 0.32 & 0.40 \\
\bottomrule
\end{tabular}
\vspace*{0.5cm}
\caption[Reconstruction Error - Evaluation: Window Length]{This table contains experimental results of the five time series discretization algorithms with respect to the reconstruction error across the four datasets. The reconstruction error is measured based on the \ac{MAE}. In this table, the mean of the \ac{MAE} across all time series of the respective dataset is presented. The time series discretization algorithms were run based on an alphabet size of $a = 5$. For the \ac{PAA}, the window lengths of $w = 2$, $w = 5$, $w = 10$, $w = 20$, and $w = 40$ were used.}
\label{tab:recon_error_window_length}
\end{table}
\subsection*{Experimental Results for Different Alphabet Sizes}
The time series discretization algorithms presented in Chapter \ref{chap:ts_discretization} were experimentally evaluated with respect to the reconstruction error based on different alphabet sizes $a \in (3,6,12,24)$ that were used for computing the breakpoints for discretization (see Table \ref{tab:recon_error_alphabet_size}). For the \ac{1d-SAX}, the corresponding alphabet sizes $a \in (2,3,6,12)$ were used for discretizing the slopes of the subsequences extracted by the \ac{PAA}. In the literature, it is recommended to use a smaller alphabet size for the extracted slopes compared to that for the extracted means \cite{1d-SAX}. Furthermore, for the subsequences extracted by the \ac{PAA}, the window lengths $w = 5$ and $w = 20$ were used for the overall experimental evaluation. For a concise presentation of the corresponding experimental results, the reconstruction error was measured based on the \ac{MAE}, since the results are qualitatively consistent when measuring the reconstruction error based on the \ac{MSE}. The mean of the MAE across all time series of the respective dataset is presented as experimental result. \newline
The overall experimental results show for all evaluated time series discretization algorithms across all evaluated datasets that the reconstruction error decreases with an increasing alphabet size. This finding is in line with the considerations in Subsection \ref{parameter_alphabet_size} about the granularity of the discretization dependent on the alphabet size. Furthermore, the findings for an alphabet size of $a = 5$ and different window lengths described in the last subsection (see Subsection \ref{findings_different_window_length}) are also indicated by these experimental results for different alphabet sizes up to an alphabet size of $a = 12$. For the increase of the evaluated alphabet size from $a = 12$ to $a = 24$, these experimental results indicate three new findings that are consistent across all evaluated datasets. \newline
The first finding is that the reconstruction error converges with an increasing alphabet size for all evaluated time series discretization algorithms. When increasing the alphabet size from $a = 12$ to $a = 24$, the reduction in the reconstruction error is less significant compared to the increase from $a = 3$ to $a = 6$ or $a = 6$ to $a = 12$. On the other hand, the reduction in the reconstruction error is most significant when increasing the alphabet size from $a = 3$ to $a = 6$. The reason for this convergence of the reconstruction error with an increasing alphabet size is based on the considerations in Subsection \ref{parameter_alphabet_size}. With an increasing alphabet size, the granularity of the discretization increases as the number of discretization intervals increases. Thus, the numerical values $f(\alpha_j) \ (1 \leq j \leq a)$ become a more accurate approximation of the points assigned to the respective discretization interval corresponding to the alphabet symbol $\alpha_j \ (1 \leq j \leq a)$ overall. The experimental results indicate that the improvement of this approximation decreases with an increasing alphabet size, since the reduction of the reconstruction error decreases. This convergence of the reconstruction error is even faster for the window length of $w = 20$ compared to $w = 5$ as the reduction in the reconstruction error from $a = 6$ to $a = 12$ is less significant for $w = 20$. \newline
Based on this finding, a recommendation for an appropriate alphabet size to handle the tradeoff described in Subsection \ref{parameter_alphabet_size} between the granularity of the discretization and the memory requirements of the discretized time series can be derived. As the most significant reduction of the reconstruction error is achieved when increasing the alphabet size from $a = 3$ to $a = 6$, an appropriate alphabet size for minimizing the reconstruction error (i.e. maximizing the granularity of the discretization) is lower bounded by $a > 3$. Moreover, as the least significant reduction of the reconstruction error is achieved when increasing the alphabet size from $a = 12$ to $a = 24$, an appropriate alphabet size for minimizing the memory requirements of the discretized time series, is upper bounded by $a \leq 12$. This upper bound can also be increased to $a = 16$, since representing a single alphabet symbol out of 12 or 16 alphabet symbols requires four bits in both cases. Thus, an appropriate alphabet size to handle the described tradeoff is bounded by $3 < a \leq 16$. \newline
The second finding is that with an increasing alphabet size, the distance of the reconstruction errors between the \ac{eSAX} and \ac{1d-SAX} one the one hand and the \ac{SAX}, \ac{aSAX}, and Persist on the other hand increases. This finding indicates that with an increasing alphabet size, the benefit of discretizing additional features like the minimum point, maximum point, and slope of subsequences increases with respect to minimizing the reconstruction error. \newline
The third finding is that the reconstruction error for the Persist converges to that of the \ac{SAX} and \ac{aSAX} with an increasing alphabet size. An explanation for this effect is that the flexibility of computing adapted breakpoints decreases as the number of breakpoints (i.e. the alphabet size) increases. Therefore, the difference in the breakpoints across different time series discretization algorithms decreases. Thus, the difference in the reconstruction errors across different time series discretization algorithms decreases as well when discretizing the same feature like the mean of subsequences.
\begin{table}[htb]
\centering
\begin{tabular}{ccccccccc} 
\toprule
& \multicolumn{4}{c}{\ac{MAE} ($w = 5$)} & \multicolumn{4}{c}{\ac{MAE} ($w = 20$)} \\
\cmidrule(lr){2-5} \cmidrule(lr){6-9}
& $a = 3$ & $a = 6$ & $a = 12$ & $a = 24$ & $a = 3$ & $a = 6$ & $a = 12$ & a = 24 \\
\midrule
\textbf{\texttt{Gaussian}} &  &  &  & \\
\ac{SAX} & 0.76 & 0.72 & 0.72 & 0.71 & 0.80 & 0.78 & 0.78 & 0.78 \\
\ac{eSAX} & 0.47 & 0.37 & 0.32 & 0.29 & 0.69 & 0.65 & 0.62 & 0.61 \\
\ac{1d-SAX} & 0.70 & 0.66 & 0.64 & 0.64 & 0.79 & 0.77 & 0.76 & 0.76 \\
\ac{aSAX} & 0.77 & 0.73 & 0.72 & 0.72 & 0.86 & 0.79 & 0.78 & 0.78 \\
Persist & 0.81 & 0.79 & 0.73 & 0.71 & 0.83 & 0.84 & 0.80 & 0.78 \\
\midrule
\textbf{\texttt{Non-Gaussian}} &  &  &  & \\
\ac{SAX} & 0.31 & 0.23 & 0.21 & 0.20 & 0.40 & 0.34 & 0.32 & 0.32 \\
\ac{eSAX} & 0.28 & 0.17 & 0.12 & 0.09 & 0.37 & 0.29 & 0.27 & 0.26 \\
\ac{1d-SAX} & 0.31 & 0.22 & 0.18 & 0.17 & 0.39 & 0.30 & 0.27 & 0.25 \\
\ac{aSAX} & 0.28 & 0.21 & 0.20 & 0.20 & 0.38 & 0.33 & 0.32 & 0.32 \\
Persist & 0.30 & 0.23 & 0.22 & 0.21 & 0.40 & 0.37 & 0.36 & 0.35 \\
\midrule
\textbf{\texttt{UCR}} &  &  &  & \\
\ac{SAX} & 0.44 & 0.34 & 0.30 & 0.29 & 0.68 & 0.65 & 0.64 & 0.64 \\
\ac{eSAX} & 0.38 & 0.23 & 0.17 & 0.14 & 0.62 & 0.56 & 0.54 & 0.53 \\
\ac{1d-SAX} & 0.39 & 0.26 & 0.20 & 0.16 & 0.57 & 0.51 & 0.47 & 0.46 \\
\ac{aSAX} & 0.43 & 0.33 & 0.30 & 0.29 & 0.71 & 0.67 & 0.66 & 0.66 \\
Persist & 0.61 & 0.38 & 0.31 & 0.29 & 0.74 & 0.67 & 0.64 & 0.64 \\
\midrule
\textbf{\texttt{Motif}} &  &  &  & \\
\ac{SAX} & 0.33 & 0.21 & 0.16 & 0.15 & 0.38 & 0.30 & 0.27 & 0.26 \\
\ac{eSAX} & 0.32 & 0.18 & 0.11 & 0.08 & 0.36 & 0.26 & 0.23 & 0.21 \\
\ac{1d-SAX} & 0.32 & 0.19 & 0.13 & 0.11 & 0.35 & 0.25 & 0.20 & 0.18 \\
\ac{aSAX} & 0.32 & 0.20 & 0.16 & 0.14 & 0.37 & 0.29 & 0.26 & 0.26 \\
Persist & 0.40 & 0.22 & 0.16 & 0.14 & 0.45 & 0.30 & 0.26 & 0.25 \\
\bottomrule
\end{tabular}
\vspace*{0.5cm}
\caption[Reconstruction Error - Evaluation: Alphabet Size]{This table contains experimental results of the five time series discretization algorithms with respect to the reconstruction error across the four datasets. The reconstruction error is measured based on the \ac{MAE}. In this table, the mean of the \ac{MAE} across all time series of the respective dataset is presented. The time series discretization algorithms were run based on the alphabet sizes of $a = 3$, $a = 6$, $a = 12$, and $a = 24$. For the \ac{PAA}, the window lengths of $w = 5$ and $w = 20$ were used.}
\label{tab:recon_error_alphabet_size}
\end{table}