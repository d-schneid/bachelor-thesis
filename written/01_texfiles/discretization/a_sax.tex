\subsection{Adaptive Symbolic Aggregate Approximation}
Similar to the fixed-breakpoints discretization algorithms, the \ac{aSAX} discretization algorithm applies amplitude discretization to the \ac{PAA} representation to obtain the discretized \ac{aSAX} representation of the original time series \cite{A_SAX}. However, for computing the breakpoints that define the discretization intervals, it employs the k-means clustering algorithm on the points of the time series that shall be discretized \cite{A_SAX}. Hence, it takes the time series data into account for discretization and computes adapted breakpoints, respectively.
\subsection*{Main Procedure}
Let $DB := \{X \mid X = x_1, ..., x_N \}$ be a time series database containing standardized time series of length $N \geq 1$. Further, define $P := \{\frac{j}{a} \mid j \in \{1, ..., a-1\}\}$ for $a \geq 2$. Applying the quantile function of the standard normal distribution to the probabilities in $P$ results in the $a$-quantiles $\beta_j$ $(1 \leq j \leq a-1)$ of the standard normal distribution. The ascending sorted list of these $a$-quantiles $B := (\beta_0, \beta_1, ..., \beta_{a-1}, \beta_a)$ with $\beta_0 = -\infty$ and $\beta_a = +\infty$ are then the breakpoints used for discretization in the \ac{SAX} based on Equation \ref{eq:SAX_Discretization} and an alphabet size of $a$ \cite{SAX_Lin_first}. \newline
The \ac{aSAX} now employs the $k$-means clustering algorithm in one dimension to adapt these breakpoints $B$, such that the adapted breakpoints $B^*$ reflect the distributions of the time series in $DB$ (see Figure \ref{fig:Diff_Breakpoints}) \cite{A_SAX}. For this, a part of the time series in $DB$ are transformed into their \ac{PAA} representations and taken as a training set \cite{A_SAX}. Let $DB_{Tr} := \{\overline{X} \mid X \in DB, \overline{X} = \overline{x}_1, ..., \overline{x}_n \}$ be such a training set, where $\overline{x}_i \ (1 \leq i \leq n)$ is the mean of the $i$-th extracted subsequence based on the \ac{PAA} with $1 \leq n \leq N$ extracted subsequences from each $X \in DB$ that is used for the training set. Then, the employed k-means clustering algorithm shown in Algorithm \ref{alg:A_SAX_k_means} uses $B$ as initial breakpoints and clusters all means $\overline{x}_i$ of all \ac{PAA} representations in $DB_{Tr}$ into $k = a$ clusters \cite{A_SAX}. Based on the computed cluster centers, it computes and eventually returns the adapted breakpoints $B^*$ (see Figure \ref{fig:k_means}). \newline
These adapted breakpoints are then used for transforming each time series $X \in DB$ into its discretized \ac{aSAX} representation \cite{A_SAX}. First, each time series that is not in $DB_{Tr}$ is also transformed into its \ac{PAA} representation. Then, the discretization for each time series is performed analogous to the \ac{SAX} based on its \ac{PAA} representation and Equation \ref{eq:SAX_Discretization}, but instead of using $B$ as breakpoints, the adapted breakpoints $B^*$ are used.
\newpage
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{discretization/a_sax/a_sax_vs_sax.pdf}
\caption[Adaptive Symbolic Aggregate Approximation - SAX vs. aSAX]{In the above two plots, 125 time series points drawn from the standard normal distribution $\mathcal{N}(0,1)$ are shown. In the below two plots, the plotted time series is composed of 50 + 50 + 25 points drawn from $\mathcal{N}(-0.45,0.1)$, $\mathcal{N}(0.45,0.1)$, and $\mathcal{N}(1,0.1)$, respectively. Hence, this time series follows a non-Gaussian distribution. On the left side of this figure, the breakpoints based on the \ac{SAX} are plotted, while on the right side, the breakpoints based on the \ac{aSAX} are plotted. The breakpoints of the \ac{aSAX} adapt to the non-Gaussian distribution by separating the points drawn from the three different distributions. On the other hand, the breakpoints of the \ac{SAX} are not able to separate these points.}
\label{fig:Diff_Breakpoints}
\end{figure}
\begin{center}
\begin{algorithm}[H]
  \SetAlgoLined
  \LinesNumbered
  \DontPrintSemicolon
  \KwIn{$k$ \tcp*[f]{number of clusters that shall be found} \newline
  		$DB_{Tr}$ \tcp*[f]{means of \ac{PAA} representations} \newline
  		$B$ \tcp*[f]{initial breakpoints} \newline
  		$\gamma > 0$ \tcp*[f]{threshold for relative training error}}
  \KwOut{$B^*$ \tcp*[f]{adapted breakpoints}}
  
  $\Delta \leftarrow \infty$ \tcp*[f]{training error of previous clustering}\;
  $B^* \leftarrow B$\;
  $C \leftarrow ()$ \tcp*[f]{cluster centers}\; 
  
  \For{$i \leftarrow 0$ \KwTo $k-1$}{
  	$b_{i} \leftarrow [ \beta_i, \beta_{i+1} )$ \tcp*[f]{$\beta_i, \beta_{i+1} \in B^*$}\;
    $n_i \leftarrow \sum_{\overline{x} \in b_i} 1$ \tcp*[f]{number of all $\overline{x} \in DB_{Tr}$ in cluster $b_i$}\;
    $s_i \leftarrow$ $\sum_{\overline{x} \in b_i} \overline{x}$ \tcp*[f]{sum of all $\overline{x} \in DB_{Tr}$ in cluster $b_i$}\;
    $c_i \leftarrow \frac{1}{n_i} s_i$ \tcp*[f]{cluster center of the $i$th cluster}\;
    $C(i) \leftarrow c_i$\;
  }
  
  \For{$i \leftarrow 1$ \KwTo $k-1$}{   
    $\beta_i \leftarrow (c_{i-1} + c_i) \ / \ 2$ \tcp*[f]{new breakpoint, $c_{i-1}, c_i \in C$}\;
    $B^*(i) \leftarrow \beta_i$\;
  }
  
  $\Delta' \leftarrow 0$ \tcp*[f]{training error of current clustering}\;
  \For{$i \leftarrow 1$ \KwTo $k$}{
    $b_{i} \leftarrow [ \beta_i, \beta_{i+1} )$ \tcp*[f]{$\beta_i, \beta_{i+1} \in B^*$}\;
    $e_i \leftarrow \sum_{\overline{x} \in b_i} (\overline{x} - c_i)^2$ \tcp*[f]{training error in cluster $b_i$, $c_i \in C$}\;
  	$\Delta' \leftarrow \Delta' \ + \ e_i$\;
  }
  
  \eIf{$\frac{\Delta - \Delta'}{\Delta} < \gamma$}{
    return $B^*$\;
  } {
    $\Delta \leftarrow \Delta'$\;
    goto line 4 \tcp*[f]{adapt clustering for lower training error}\;
  }
  
  \caption[Adaptive Symbolic Aggregate Approximation - k-means]{For the \ac{aSAX}, the k-means clustering algorithm clusters the means of the \ac{PAA} representations of the given training time series \cite{A_SAX}. Based on the found clustering, it computes adapted breakpoints that are used to transform time series into their respective \ac{aSAX} representation.}
  \label{alg:A_SAX_k_means}
\end{algorithm}
\end{center}
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{discretization/a_sax/k_means.pdf}
\caption[Adaptive Symbolic Aggregate Approximation - Adapted Breakpoints]{The adapted breakpoints that are computed by the k-means clustering algorithm are the midpoints of each two adjacent cluster centers (yellow crosses) \cite{A_SAX}. These breakpoints adapt based on the distribution of the time series that shall be discretized. Points with the same color belong to the same cluster.}
\label{fig:k_means}
\end{figure}
\subsection*{Modifications} \label{a_sax_modfications}
Instead of computing adapted breakpoints $B^*$ for a whole time series database $DB$, the \ac{aSAX} can also be used for computing individual adapted breakpoints for each time series $X \in DB$. This can be done by transforming each $X \in DB$ into its \ac{PAA} representation $\overline{X}$ and applying the k-means clustering algorithm in Algorithm \ref{alg:A_SAX_k_means} for each \ac{PAA} representation $\overline{X}$ individually. With this modification, each time series $X \in DB$ has individually adapted breakpoints that solely reflect its own distribution. However, the time and memory requirements for computing and storing these individual breakpoints are higher compared to computing adapted breakpoints for a whole time series database. \newline
A further modification is to use a part of the time series $DB' \subset DB$ as a training set without transforming them into their \ac{PAA} representations \cite{A_SAX}. So, instead of clustering the means of their \ac{PAA} representations, the original points of the time series in the training set would be clustered for computing the adapted breakpoints $B^*$. While the k-means clustering algorithm in Algorithm \ref{alg:A_SAX_k_means} does not need to be modified, except for using $DB'$ instead of $DB_{Tr}$ as input, the recommended approach in the literature is to use $DB_{Tr}$ as the training set \cite{A_SAX}. The advantage of using $DB_{Tr}$ instead of $DB'$ is that the k-means clustering algorithm needs to cluster less points for $n < N$. Thus, its time requirements are lower.
\subsection*{Unsupervised Discretization}
Since the \ac{aSAX} computes the breakpoints used for discretization based on the k-means clustering algorithm, it can also be used for unsupervised discretization \cite{Unsupervised_Discretization}. This means that the alphabet size $a$ is no longer an input parameter of the \ac{aSAX}, but can be determined intrinsically. In this case, the \ac{aSAX} only requires a single input parameter which is the window length $1 \leq w \leq N$ that is used for the \ac{PAA}. \newline
The question that now remains is how to intrinsically determine an appropriate alphabet size for the \ac{aSAX}. One possibility is to apply a decision criterion for the k-means clustering algorithm shown in Algorithm \ref{alg:A_SAX_k_means} \cite{Unsupervised_Discretization}. Let $A^* := (a_{min}, ..., a_{max})$ be an ascending sorted list of different alphabet sizes, where $a_{min} \geq 2$ is the minimum and $a_{max} \geq a_{min}$ is the maximum alphabet size. Then, Algorithm \ref{alg:A_SAX_k_means} can be run for each alphabet size in $A^*$ while computing the value of the decision criterion for the corresponding resulting clustering. This way, the best alphabet size $a^* \in A^*$ is the one that was used for the resulting clustering that achieved the best value based on the used decision criterion. Thus, the breakpoints that were computed for the alphabet size $a^*$ can then be used for discretizing the respective time series. \newline
It still remains open what decision criterion should be used. One possibility is to use the \ac{SSE} that is computed with the $for$-loop starting at line 16 in Algorithm \ref{alg:A_SAX_k_means}. Using the \ac{SSE}, the elbow method can be applied for finding $a^*$ by plotting the value of the \ac{SSE} that was achieved for each alphabet size $a \in A^*$ the k-means clustering algorithm was run (see Figure \ref{fig:elbow_method}) \cite{Elbow_Method}.
\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{discretization/a_sax/a_sax_unsupervised.pdf}
\caption[Adaptive Symbolic Aggregate Approximation - Elbow Method]{The time series in the left plot is composed of 50 + 50 + 25 points drawn from $\mathcal{N}(-0.4,0.1)$, $\mathcal{N}(0.4,0.1)$, and $\mathcal{N}(1,0.1)$, respectively, with a subsequent standardization of the composed time series. Then, the k-means clustering algorithm shown in Algorithm \ref{alg:A_SAX_k_means} is run for different alphabet sizes $a \in (2, ..., 10)$ to cluster the points of the plotted time series. The corresponding values of the \ac{SSE} are shown in the right plot. Using the elbow method, the plotted time series should be discretized based on an alphabet size of $a^* = 3$. The corresponding breakpoints for discretization in the left plot are adapted to the three distributions the time series points were drawn from.}
\label{fig:elbow_method}
\end{figure}
\subsection*{Time Complexity}
The first step of the \ac{aSAX} without any modifications is to compute the initial parameters for the k-means clustering algorithm in Algorithm \ref{alg:A_SAX_k_means}. The computation of $DB_{Tr}$ involves transforming a fixed number $0 < r \leq |DB|$ of the time series in $DB$ into their \ac{PAA} representations. This can be done in $\mathcal{O}(r \cdot N) = \mathcal{O}(N)$, since the time complexity of the \ac{PAA} for one time series is $\mathcal{O}(N)$. Further, as explained for the \ac{SAX}, the time complexity of computing the initial breakpoints $B$ is $\mathcal{O}(1)$. Hence, the total time complexity of computing the initial parameters for the k-means clustering algorithm is $\mathcal{O}(N)$. \newline
The next step is the computation of the adapted breakpoints $B^*$ based on the k-means clustering algorithm. This computation has a total time complexity of $\mathcal{O}(iters \cdot k \cdot n) = \mathcal{O}(iters \cdot n)$, where $k = a$ is the given number of clusters that shall be computed and $iters \geq 1$ is the number of iterations until the algorithm converges. This total time complexity follows, because for a single iteration, the first and third $for$-loop of Algorithm \ref{alg:A_SAX_k_means} both have a time complexity of $\mathcal{O}(k \cdot n) = \mathcal{O}(n)$, while the second $for$-loop has a time complexity of $\mathcal{O}(k) = \mathcal{O}(1)$. \newline
The final discretization step of the \ac{aSAX} is analogous to the \ac{SAX} except for using the computed adapted breakpoints $B^*$ instead of $B$. Therefore, the time complexity for this step is $\mathcal{O}(n)$ as described for the \ac{SAX}. \newline
Thus, the total time complexity of the \ac{aSAX} without any modifications is $\mathcal{O}(N) + \mathcal{O}(iters \cdot n) + \mathcal{O}(n) = \mathcal{O}(N + iters \cdot n)$.

